{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis in Real Estate\n",
    "This is the part of Inronhack Mid-term Bootcamp project. \n",
    "\n",
    "## Objective\n",
    "\n",
    "- The following project will idenfity the factors that influnce to the selling price of properties.\n",
    "- The following project will predict the houing price using machine learning algorithms (Linear Regression, KNNNeighbour Regressor & Random Forest Regressor)\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- The dataset consists of information on 22,000 properties. \n",
    "- The dataset consists of historic data of houses sold between May 2014 to May 2015 in King County WA.\n",
    "- These are the definitions of data points provided:\n",
    "\n",
    "| Column Name   | Description                                                                 |\n",
    "| ------------- | --------------------------------------------------------------------------- |\n",
    "| id            | ID of the house                                                             |\n",
    "| date          | Date the house was sold                                                     |\n",
    "| bedrooms      | Number of bedrooms                                                          |\n",
    "| bathrooms     | Number of bathrooms                                                         |\n",
    "| sqft_living   | Square footage of the home                                                   |\n",
    "| sqft_lot      | Square footage of the lot                                                   |\n",
    "| floors        | Total floors in the house                                                   |\n",
    "| waterfront    | House which has a view to a waterfront                                      |\n",
    "| view          | Has been viewed                                                             |\n",
    "| condition     | How good the condition is overall                                           |\n",
    "| grade         | Overall grade given to the housing unit, based on King County grading system|\n",
    "| sqft_above    | Square footage of house apart from basement                                 |\n",
    "| sqft_basement | Square footage of the basement                                              |\n",
    "| yr_built      | Built Year                                                                  |\n",
    "| yr_renovated  | Year when the house was renovated                                           |\n",
    "| zipcode       | Zip code                                                                    |\n",
    "| lat           | Latitude coordinate                                                         |\n",
    "| long          | Longitude coordinate                                                        |\n",
    "| sqft_living15 | Living room area in 2015 (implies-- some renovations)                       |\n",
    "| sqft_lot15    | Lot size area in 2015 (implies-- some renovations)                          |\n",
    "| price         | Price of the house                                                          |\n",
    "| district      | District where the house is located                                         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure\n",
    "1. Import libraries & Load Dataset\n",
    "2. Overview of Dataset\n",
    "3. Data Cleaning\n",
    "4. Exploratory Data Analysis (EDA)\n",
    "5. Data Modelling: Linear Regressor, KNN Regressor, Random Forest Regressor\n",
    "6. Cross examination of modesl\n",
    "7. Feature importance\n",
    "8. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries & load dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dooinnkim/ironhack_da_may_2023/data_mid_bootcamp_project_regression/regression_data.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-424d054f135b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/dooinnkim/ironhack_da_may_2023/data_mid_bootcamp_project_regression/regression_data.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dooinnkim/ironhack_da_may_2023/data_mid_bootcamp_project_regression/regression_data.xls'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('/Users/dooinnkim/ironhack_da_may_2023/data_mid_bootcamp_project_regression/regression_data.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Dataset\n",
    "- There are 21,597 entries in this dataset (i.e., houses for sale).\n",
    "- The dataset includes various features such as the number of bedrooms and bathrooms, living area (in square feet), lot size (in square feet), number of floors, whether the house is a waterfront property, view quality, condition and grade of the house, above ground living area (in square feet), basement area (in square feet), the year the house was built and renovated, zip code, latitude, longitude, the living area in 2015 (in square feet), the lot size in 2015 (in square feet), and the house price.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- **Bedrooms**: The average house has around 3.37 bedrooms, but the number can vary greatly, with a standard deviation of about 0.93. The minimum number of bedrooms is 1, and the maximum is 33.\n",
    "\n",
    "- **Bathrooms**: Houses have on average 2.12 bathrooms with a standard deviation of 0.77. The minimum number of bathrooms is 0.5 (likely a toilet and sink but no shower or tub), while the maximum is 8.\n",
    "\n",
    "- **Sqft_living**: The average living space is about 2,080 square feet, with a standard deviation of 918. The smallest house has 370 square feet, while the largest has 13,540 square feet.\n",
    "\n",
    "- **Sqft_lot**: The average lot size is 15,099 square feet (roughly a third of an acre), but the size varies greatly (standard deviation is 41,412) with the largest lot being 1,651,359 square feet (almost 38 acres).\n",
    "\n",
    "- **Floors**: The average number of floors in a house is approximately 1.49, and houses in the dataset have between 1 and 3.5 floors.\n",
    "\n",
    "- **Waterfront**: The mean of this feature is close to 0, suggesting that most houses are not on the waterfront.\n",
    "\n",
    "- **Price**: House prices average around $540,296, with a standard deviation of $367,368. The cheapest house costs $78,000,  while the most expensive one is  $7,700,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there are no missing values and duplicated rows which means that it's already cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althougg the dataset seems already cleanded, we will create additional columns: district and house_age for the following reasons:\n",
    "- **District**: We assume that zipcode and lattitude and longtitude would not the recognizeable factor that people consider when buying properties. Instead, the name of district would influence on the purchase decision (which is more common convention). We add 'district' column using uszipcode python library.\n",
    "- **house_age**: assuming that the age of house is one of the factros that would influence to the selling price.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using uszipcode, a python geo library\n",
    "from uszipcode import SearchEngine\n",
    "\n",
    "\n",
    "search = SearchEngine()\n",
    "\n",
    "\n",
    "def get_state_by_zip(zipcode):\n",
    "    zipcode_info = search.by_zipcode(zipcode)\n",
    "    if zipcode_info:\n",
    "        return zipcode_info.state\n",
    "\n",
    "\n",
    "def get_district_by_zip(zipcode):\n",
    "    zipcode_info = search.by_zipcode(zipcode)\n",
    "    if zipcode_info:\n",
    "        return zipcode_info.major_city\n",
    "\n",
    "\n",
    "# Create a new 'District' column\n",
    "df['district'] = df['zipcode'].apply(get_district_by_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'age of house' column\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df['house_age'] = df['year'] - df['yr_built']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EDA (Exploratory Data Analysis)\n",
    "\n",
    "We've briefly seen the summary of the dataset in Data Overview part. This part will investigate in dept of the respective features and see if there are any useful business insights for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove unecessary columns for the analysis\n",
    "df = df.drop(columns=['id', 'date', 'year'])\n",
    "\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select only numeric features from the DataFrame\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Fit and transform the numeric features with StandardScaler\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns)\n",
    "\n",
    "# Create boxplots\n",
    "scaled_df.boxplot(figsize=(20, 6))\n",
    "plt.title(\"Boxplots of Scaled Numerical Columns\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsevations on Distribution\n",
    "\n",
    "- **bedrooms**: Most houses have between 2 and 5 bedrooms, with 3 bedrooms being the most common.\n",
    "\n",
    "- **bathrooms**: Most houses have between 1 and 3 bathrooms. Houses with 2.5 bathrooms (a common configuration with 2 full bathrooms and a half bathroom) are particularly common.\n",
    "\n",
    "- **sqft_living**: The square footage of living spaces in the houses is right-skewed, meaning most houses have smaller living spaces, but there are a few houses with very large living spaces.\n",
    "\n",
    "- **sqft_lot**: Similarly, the square footage of lots is also right-skewed. Most houses have smaller lots, but a few houses have very large lots.\n",
    "\n",
    "- **floors**: Many houses have 1 or 2 floors. There are some houses with 1.5, 2.5, or 3 floors, indicating that some houses have a loft or a half floor.\n",
    "\n",
    "- **waterfron**t: Almost all houses do not have a waterfront view, as indicated by the bar at 0.\n",
    "\n",
    "- **view**: Most houses have not been viewed, but some houses have been viewed multiple times.\n",
    "\n",
    "- **condition**: Most houses are in condition 3 or 4. Very few houses are in poor condition (1) or excellent condition (5).\n",
    "\n",
    "- **grade**: The grading of the houses seems to follow a normal distribution, with most houses having a grade around 7.\n",
    "\n",
    "- **sqft_above**: The square footage of house apart from basement is right-skewed, with most houses having smaller areas, but a few houses having very large areas.\n",
    "\n",
    "- **sqft_basement**: Many houses do not have a basement (indicated by the bar at 0). Among the houses that do have a basement, the square footage of the basement is right-skewed.\n",
    "\n",
    "- **yr_built**: The year the houses were built is roughly uniformly distributed across the years, with a slight increase in more recent years.\n",
    "\n",
    "- **yr_renovated**: Most houses have not been renovated (indicated by the bar at 0). Among the houses that have been renovated, the year of renovation is right-skewed.\n",
    "\n",
    "- **zipcode**: The zip codes appear to be uniformly distributed, indicating that the houses are spread out across the zip codes.\n",
    "\n",
    "- **lat and long**: The latitude and longitude indicate the geographical location of the houses. There seem to be clusters of houses at certain latitudes and longitudes.\n",
    "\n",
    "- **sqft_living15 and sqft_lot15**: The square footage of the living room area in 2015 and the lot size area in 2015 are both right-skewed, similar to sqft_living and sqft_lot.\n",
    "\n",
    "- **price**: The price of the houses is right-skewed. Most houses are priced lower, but there are a few houses with very high prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Let's also check the outliers of each features using IQR method. Treating outliers is important for the following several reasons:\n",
    "\n",
    "1. **Accuracy**: Outliers can significantly affect the mean and standard deviation of your data, which are used in a variety of statistical tests and machine learning models. This can lead to inaccurate results. For example, in a linear regression model, a single outlier can dramatically change the line of best fit, leading to inaccurate predictions.\n",
    "\n",
    "2. **Model Performance**: Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in the data can cause problems with these algorithms, leading to poor performance.\n",
    "\n",
    "3. **Data Quality**: Outliers can sometimes be the result of errors in data collection or entry. Identifying and addressing these outliers can help improve the quality of your data.\n",
    "\n",
    "3. **Interpretability**: In some cases, outliers can make it more difficult to understand the underlying patterns and trends in your data. Treating these outliers can make your data analysis and visualizations more interpretable.\n",
    "\n",
    "4. **Assumptions**: Many statistical procedures assume a normal distribution and outliers can violate this assumption, leading to incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_dict = {}\n",
    "\n",
    "for col in numeric_df.columns:\n",
    "\n",
    "    Q1 = numeric_df[col].quantile(0.25)\n",
    "    Q3 = numeric_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outliers = numeric_df[(numeric_df[col] < (Q1 - 1.5 * IQR)) | (numeric_df[col] > (Q3 + 1.5 * IQR))][col]\n",
    "    \n",
    "    outlier_percentage = round((len(outliers) / len(numeric_df[col]) * 100),2)\n",
    "    \n",
    "    outliers_dict[col] = {'outliers_count': len(outliers), 'outliers_percentage': outlier_percentage}\n",
    "    outliers_df = pd.DataFrame.from_dict(outliers_dict, orient='index')\n",
    "\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are possible suggestions for treating the outliers in prior to training our machine learning model.\n",
    "\n",
    "- **'bedrooms'**: 530 outliers. If these represent homes with a very high number of bedrooms, they might be valid entries representing large houses or mansions. Investigate the actual values and consider keeping them or using winsorizing.\n",
    "\n",
    "- **'bathrooms'**: 561 outliers. Similar to 'bedrooms', these could be valid entries representing large houses. Review the actual values and consider keeping them or using winsorizing.\n",
    "\n",
    "- **'sqft_living'**: 571 outliers. These might represent particularly large or small houses. Consider a log transformation to handle the skewness of the data.\n",
    "\n",
    "- **'sqft_lot'**: 2419 outliers. These might represent properties with particularly large lot sizes. A log transformation could be useful here too.\n",
    "\n",
    "- **'waterfront'**: 163 outliers. This is likely a binary feature indicating whether the property is waterfront or not. Outliers might simply be the less common class (e.g., waterfront properties). If this is the case, no outlier handling is needed.\n",
    "\n",
    "- **'view'**: 2122 outliers. If this feature represents the number of views a property has had, outliers could be properties that are particularly popular or unpopular. Depending on the distribution, a transformation or winsorizing might be appropriate.\n",
    "\n",
    "- **'condition'**: 29 outliers. This is likely a categorical variable, and \"outliers\" are probably just less common conditions. No outlier handling is likely needed.\n",
    "\n",
    "- **'grade'**: 1905 outliers. If this is a grading system for the quality of a house, outliers may be houses that are extremely high or low quality. These could be important to keep as they may have a significant impact on house prices.\n",
    "\n",
    "- **'sqft_above', 'sqft_basement'**: These could be handled similarly to 'sqft_living'. Consider a log transformation.\n",
    "\n",
    "- **'yr_renovated'**: 914 outliers. These might be houses that were recently renovated. If many houses have a value of 0 (indicating no renovation), this could lead to a skewed distribution. One approach could be to turn this into a binary 'was_renovated' feature.\n",
    "\n",
    "- **'lat', 'long'**: 2 and 255 outliers, respectively. These are coordinates and outliers may represent properties that are geographically distant from others. It might be worth keeping these as they could represent different real estate markets.\n",
    "\n",
    "- **'sqft_living15', 'sqft_lot15'**: Outliers in these features might be handled in the same way as 'sqft_living' and 'sqft_lot', possibly with a log transformation.\n",
    "\n",
    "- **'price'**: 1158 outliers. These are likely high-value houses. Because the goal is likely to predict this variable, it's important to handle outliers carefully. A log transformation is a common choice for skewed target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=df.corr(method='pearson')\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Correlation Matrix\n",
    "\n",
    "- **sqft_living** has a high positive correlation with **price (0.701917)**, meaning that houses with more living space tend to be more expensive. This variable also has strong correlations with **bathrooms (0.755758)**, **grade (0.762779)**, and **sqft_above (0.876448)**, suggesting that larger houses tend to have more bathrooms, higher grades, and more above ground space.\n",
    "\n",
    "- **grade** and **sqft_above** also have significant positive correlations with **price (0.667951 and 0.605368 respectively)**, suggesting that the quality of the house and the above ground space are important factors in determining the price of a house.\n",
    "\n",
    "\n",
    "- **floors** and **condition** have a moderate negative correlation (-0.264075), implying that houses with more floors tend not to be in as good condition, or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following features reveal particularly strong correlated relationship with price. This suggests that the size and quality of a house (as measured by square footage and grade) are important factors that influence its price:\n",
    "\n",
    "- **sqft_living**: 0.70\n",
    "- **grade**: 0.67\n",
    "- **sqft_above**: 0.61\n",
    "- **sqft_living15**: 0.59\n",
    "- **bathrooms**: 0.53\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for 'price'\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['price'], bins=30, kde=True)\n",
    "plt.title('Distribution of Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot scatterplots for 'price' vs highly correlated features\n",
    "highly_corr_features = ['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']\n",
    "for feature in highly_corr_features:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.scatterplot(data=df, x=feature, y='price')\n",
    "    plt.title(f'Price vs {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some observations from the visualizations of **Price vs strongly correalted features**:\n",
    "\n",
    "- **Price Distribution**: The price distribution is right-skewed, indicating that while most houses are priced on the lower end, there are a few houses with very high prices.\n",
    "\n",
    "- **Price vs Square Footage of Living Area**: There is a positive correlation between the square footage of the living area (sqft_living) and the price, and the plots appears quite linear relation which makes sense as larger houses tend to be more expensive.\n",
    "\n",
    "- **Price vs Grade**: The grade of a house also seems to be positively correlated with its price. Higher-graded houses appear to be more expensive. This again is logical as better quality houses are expected to fetch higher prices.\n",
    "\n",
    "- **Price vs Square Footage Above Ground**: The square footage of house apart from basement (sqft_above) also shows a positive correlation with price. Larger above-ground living spaces tend to increase the price of a house.\n",
    "\n",
    "- **Price vs Square Footage of Living Area in 2015**: The size of the living area in 2015 (sqft_living15) also seems to affect the price, although there's a bit of spread in the data. Larger living areas in 2015 tend to be associated with higher prices.\n",
    "\n",
    "- **Price vs Number of Bathrooms**: The number of bathrooms in a house (bathrooms) is also positively correlated with its price. Houses with more bathrooms tend to be more expensive.\n",
    "\n",
    "These visualizations and observations provide some initial insights into the factors that may affect house prices. However, these are just bivariate relationships. A multivariate analysis or a machine learning model would be needed to better understand and predict house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Categorical Features relative to Price\n",
    "\n",
    "We will analysse the catergorical features in relation to the price to identify what features could impact on the selling price. First, we would need to create new addiontioal columns 'was_renovated', 'has_basement' which are the binary features if the house was renovated and has basement'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['was_renovated'] = df['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\n",
    "df['has_basement'] = df['sqft_basement'].apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.copy()\n",
    "cols_to_convert = ['waterfront', 'condition', 'grade','was_renovated','has_basement']\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_cat[col] = df_cat[col].astype(str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in df_cat.select_dtypes(include=['object']).columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    order = df_cat[column].value_counts().index\n",
    "    sns.countplot(x=column, data=df_cat, order=order)\n",
    "    plt.title(f'Countplot of {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    count_table = df_cat[column].value_counts()\n",
    "    print(count_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in df_cat.select_dtypes(include=['object']).columns:\n",
    "    plt.figure(figsize=(10, 4)) \n",
    "    sns.barplot(x=column, y='price', data=df_cat)\n",
    "    plt.title(f'Average price per {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    avg_price = pd.concat([df_cat[column], df['price']], axis=1).groupby(column).mean()\n",
    "    print(avg_price.sort_values(by='price',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsevations on Categorical features relative to Price\n",
    "\n",
    "- **Waterfront**: Houses with a waterfront are on average significantly more expensive than those without. The average price of houses with a waterfront is about 1.66 million, while those without are about 531,762.\n",
    "\n",
    "- **Condition**: The condition of a house also affects its average price. Houses in condition '5' (assuming '5' represents the best condition) have an average price of about 612,578, while those in condition '1' have an average price of around 341,067. The table suggests that, generally, houses in better condition fetch higher prices.\n",
    "\n",
    "- **Grade**: The house prices also seem to increase with the grade of the house. Houses with a grade of '13' have the highest average price at approximately 3.71 million. On the other end of the spectrum, houses with a grade of '4' have an average price of around 212,002.\n",
    "\n",
    "- **District**: The district where the house is located also significantly affects the average price. Houses in Medina have the highest average price of around 2.16 million, while those in Federal Way have the lowest average price of approximately 289,391.\n",
    "\n",
    "- **Was_renovated**: Houses that have been renovated are generally more expensive than those that haven't. The average price of houses that were renovated is about $760,629, while those that were not renovated have an average price of around 530,560.\n",
    "\n",
    "- **Has_basement**: Having a basement seems to add value to a house as well. Houses with a basement have an average price of approximately 622,518, while those without a basement have an average price of about 487,069."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#encoding\n",
    "df_model = pd.get_dummies(df_model)\n",
    "\n",
    "\n",
    "X = df_model.drop(columns=['price'])\n",
    "Y = df_model['price']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "models = [LinearRegression(), KNeighborsRegressor(), RandomForestRegressor()]\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=[\"Model\", \"r2\", \"mse\", \"mae\"])\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "\n",
    "    metrics_dict = {\"Model\": str(type(model).__name__), \"r2\": r2, \"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "    metrics_df = metrics_df.append(metrics_dict, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.regplot(x=Y_test, y=predictions, line_kws={\"color\": \"red\"})\n",
    "    \n",
    "    plt.title(f'Regression Plot: Actual vs Predicted values for {str(type(model).__name__)}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print(metrics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "\n",
    "importances = 100.0 * (importances / np.sum(importances))\n",
    "\n",
    "\n",
    "importances, feature_names = zip(*((importance, feature) for importance, feature in zip(importances, feature_names) if importance > 0.1))\n",
    "\n",
    "importances = np.array(importances)\n",
    "feature_names = np.array(feature_names)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(20, 13))\n",
    "barplot = plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "\n",
    "plt.xticks(range(len(importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance (%)\")\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "\n",
    "for rect in barplot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance (%)': importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance (%)', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_model_std = df_model.copy()\n",
    "\n",
    "\n",
    "X = df_model_std.drop(columns=['price'])\n",
    "Y = df_model_std['price']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) \n",
    "\n",
    "\n",
    "\n",
    "models = [LinearRegression(), KNeighborsRegressor(), RandomForestRegressor()]\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=[\"Model\", \"r2\", \"mse\", \"mae\"])\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "\n",
    "    metrics_dict = {\"Model\": str(type(model).__name__), \"r2\": r2, \"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "    metrics_df = metrics_df.append(metrics_dict, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.regplot(x=Y_test, y=predictions, line_kws={\"color\": \"red\"})\n",
    "    \n",
    "    plt.title(f'Regression Plot: Actual vs Predicted values for {str(type(model).__name__)}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "metrics_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Log Transform\n",
    "We will first treat the outliers of each features according to the suggestion that we have discussed in the previous part.\n",
    "\n",
    "We will log transform the following features : 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15', 'price'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_log = df_model.copy()\n",
    "\n",
    "columns_to_transform = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15', 'price']\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    df_model_log[col] = np.log1p(df_model_log[col])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_model_log.drop(columns=['price'])\n",
    "Y = df_model_log['price']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "models = [LinearRegression(), KNeighborsRegressor(), RandomForestRegressor()]\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=[\"Model\", \"r2\", \"mse\", \"mae\"])\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "\n",
    "    metrics_dict = {\"Model\": str(type(model).__name__), \"r2\": r2, \"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "    metrics_df = metrics_df.append(metrics_dict, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.regplot(x=Y_test, y=predictions, line_kws={\"color\": \"red\"})\n",
    "    \n",
    "    plt.title(f'Regression Plot: Actual vs Predicted values for {str(type(model).__name__)}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "metrics_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is slight improvement in accuracy by log transform the features. Let's further check if we can improve better by featrue selection to reduce multicolinearity detected by correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Slelection using coef\n",
    "df_model.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Multi-Colinearity between features:\n",
    "correlations_matrix = df_model.corr()\n",
    "correlations_matrix = correlations_matrix[((correlations_matrix > .8) | (correlations_matrix < -.8))]\n",
    "correlations_matrix.fillna(0)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_matrix = correlations_matrix.fillna(0)\n",
    "mask = np.zeros_like(correlations_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(40, 30))\n",
    "ax = sns.heatmap(correlations_matrix, mask=mask, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove multicorrelinerity. Long and lat can replace zipcode and district.\n",
    "df_model = df_model.drop(['sqft_above','zipcode','house_age','yr_renovated','sqft_basement'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_log = df_model.copy()\n",
    "\n",
    "columns_to_transform = ['sqft_living', 'sqft_lot', 'sqft_living15', 'sqft_lot15', 'price']\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    df_model_log[col] = np.log1p(df_model_log[col])\n",
    "\n",
    "    \n",
    "    \n",
    "# X, Y split\n",
    "X = df_model_log.drop(columns=['price'])\n",
    "Y = df_model_log['price']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "#Model fitting\n",
    "\n",
    "models = [LinearRegression(), KNeighborsRegressor(), RandomForestRegressor()]\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=[\"Model\", \"r2\", \"mse\", \"mae\"])\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "\n",
    "    metrics_dict = {\"Model\": str(type(model).__name__), \"r2\": r2, \"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "    metrics_df = metrics_df.append(metrics_dict, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.regplot(x=Y_test, y=predictions, line_kws={\"color\": \"red\"})\n",
    "    \n",
    "    plt.title(f'Regression Plot: Actual vs Predicted values for {str(type(model).__name__)}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['mae'] = np.expm1(metrics_df['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 7.Cross-examination of models\n",
    "\n",
    "In this data project, the effect of varying preprocessing techniques on the performance of three machine learning models - Linear Regression, KNeighborsRegressor, and RandomForestRegressor - was examined. Here's an analysis of the results obtained from each preprocessing stage:\n",
    "\n",
    "1. **Initial Model (Post Basic Data Cleaning)**: Upon executing the models with basic cleaned data, RandomForestRegressor emerged as the superior model according to all three metrics (r2, mse, mae). LinearRegression produced satisfactory results, while the KNeighborsRegressor underperformed, likely due to the inherent sensitivity of this algorithm to high-dimensionality and non-normalized data.\n",
    "\n",
    "\n",
    "2. **Model with Standard Scaling**: The incorporation of standard scaling enhanced the performance of all models. The most notable improvement was observed in the KNeighborsRegressor model, with substantial growth in all metrics. The reason for this improvement lies in the core functionality of the KNN algorithm, which computes the distance between instances and is consequently sensitive to feature scaling. There was also a marginal improvement in RandomForestRegressor, confirming its position as the top-performing model.\n",
    "\n",
    "\n",
    "3. **Model with Log Transformation**: The models showed significant improvements after implementing a log transformation on the house price and size. This transformation notably impacted the Linear Regression model, indicating the possibility of a multiplicative rather than additive relationship between price and features. Log transformation aids in mitigating the effects of outliers and allows the model to better understand exponential growth. However, we must bear in mind that the mse and mae metrics are now operating on a log scale, rendering them not directly comparable with prior values.\n",
    "\n",
    "\n",
    "4. **Model with Log Transformation & Reduced Multicollinearity**: Multicollinearity reduction, which combats high correlation between predictors, generally augments model stability and interpretability. When implemented alongside log transformation, it resulted in a minor reduction in Linear Regression model performance, possibly due to loss of some information when highly correlated features were removed. However, the KNeighborsRegressor model displayed an uptick in performance, while RandomForestRegressor's performance was nearly unaltered. In spite of these modifications, RandomForestRegressor maintains its position as the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Normalize the importance values and convert to percentage\n",
    "importances = 100.0 * (importances / np.sum(importances))\n",
    "\n",
    "# Filter importances and features based on the condition of importances > 0.1%\n",
    "importances, feature_names = zip(*((importance, feature) for importance, feature in zip(importances, feature_names) if importance > 0.1))\n",
    "\n",
    "importances = np.array(importances)\n",
    "feature_names = np.array(feature_names)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(20, 13))\n",
    "barplot = plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "\n",
    "plt.xticks(range(len(importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance (%)\")\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Adding the percentage labels on top of each bar\n",
    "for rect in barplot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features grade, lat, and sqft_living have the highest importance values according to the Random Forest model, suggesting they play a significant role in predicting the house prices. These variables likely have strong relationships with the house price:\n",
    "\n",
    "- grade (33.94%): This probably refers to the grading given based on the King County grading system. Higher grade houses are usually of higher quality and have better finishes, hence more expensive.\n",
    "- lat (29.45%): This refers to the latitude coordinate of the house. Location (north or south) can have a big impact on the price of a house. Certain latitudes may correspond to more desirable locations, such as being closer to city centers or better schools, thus driving up house prices.\n",
    "- sqft_living (19.20%): This is the square footage of the apartment interior living space. Larger houses typically cost more, so it's expected this feature is important.\n",
    "\n",
    "- The rest of the features contribute less to the model, with importance below 6%, indicating that while they do have some effect on the price of the house, they are not as significant as the top three. However, it's important to remember that even features with lower importance can have significant effects in specific contexts or when interacting with other features. For example, waterfront may have a huge effect on price for houses in certain locations, even though its overall importance is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- According to the Forest Random Regressor result, Grade, location, size are the main deciding factors for the price of property. \n",
    "- We still do not have specific info about grading system, so we were able to get specific stardard what broughts the higher price. Assuming that bigger size, better location (e.g. good education environment, commercial center, transportation etc.) would be ther external factors that are not existed in the dataset. \n",
    "- Thus, we would need more infomation like the transportation, education, low crimanal rate, something more social factors also need to be considered to properly estimate the size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
